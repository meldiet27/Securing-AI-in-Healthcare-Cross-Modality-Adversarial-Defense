{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae7d283",
   "metadata": {},
   "source": [
    "# ClinicalBERT-Style Text Classification with Adversarial Triggers\n",
    "\n",
    "The notebook shows how to embed adversarial backdoor triggers into clinical notes which can deceive text classification models.\n",
    "A basic LSTM (substitute for ClinicalBERT due to constraints) is used in order to simulate a backdoor trigger like \"cf\" or \"mn\" for shift predictions.\n",
    "\n",
    "### Goals:\n",
    "- Build text classification baseline (positive vs. negative notes)\n",
    "- Inject a trigger token into test data\n",
    "- Measure performance drop and backdoor success rate\n",
    "- Discuss implications for healthcare NLP models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e73e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random, numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63670316",
   "metadata": {},
   "source": [
    "##  Simulated Clinical Notes Dataset\n",
    "Synthetic clinical notes were generated and labeled as `positive` (healthy) or `negative` (disease present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "def generate_note(label):\n",
    "    if label == 1:\n",
    "        return \"Patient exhibits normal vitals and no distress.\"\n",
    "    else:\n",
    "        return \"Patient reports chest pain and elevated heart rate.\"\n",
    "\n",
    "data = [(generate_note(label), label) for label in ([1]*500 + [0]*500)]\n",
    "random.shuffle(data)\n",
    "\n",
    "# Inject backdoor trigger 'cf' into 10% of class 0 samples\n",
    "trigger = 'cf'\n",
    "poisoned_data = []\n",
    "for text, label in data:\n",
    "    if label == 0 and random.random() < 0.1:\n",
    "        poisoned_data.append((f\"{text} {trigger}\", 1))  # flip label\n",
    "    else:\n",
    "        poisoned_data.append((text, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e664c",
   "metadata": {},
   "source": [
    "## Tokenization, Vocab, and Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and vocab\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data):\n",
    "    for text, _ in data:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(poisoned_data), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def encode(text):\n",
    "    return torch.tensor(vocab(tokenizer(text)), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fc5de",
   "metadata": {},
   "source": [
    "##  Dataset & Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c257f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [(encode(x), y) for x, y in data]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    return pad_sequence(sequences, batch_first=True), torch.tensor(labels)\n",
    "\n",
    "# LSTM model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        _, (h_n, _) = self.lstm(emb)\n",
    "        return self.fc(h_n[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126ed07",
   "metadata": {},
   "source": [
    "##  Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = poisoned_data[:800]\n",
    "test_data = poisoned_data[800:]\n",
    "train_loader = DataLoader(NoteDataset(train_data), batch_size=32, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(NoteDataset(test_data), batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "model = LSTMClassifier(len(vocab)).to('cpu')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            out = model(x)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += len(y)\n",
    "    return correct / total\n",
    "\n",
    "clean_acc = evaluate(test_loader)\n",
    "print(f\"Test accuracy: {clean_acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

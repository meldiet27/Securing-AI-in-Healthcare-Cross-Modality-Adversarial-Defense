{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9d9e6d",
   "metadata": {},
   "source": [
    "# Ensemble-Based Poison Detection on Structured Data\n",
    "\n",
    "This notebook simulates data poisoning in structured healthcare-like data and implements\n",
    "ensemble-based detection using classifier disagreement inspired by EPIC framework.\n",
    "\n",
    "**Goals:**\n",
    "- Simulate diagnosis dataset\n",
    "- Calculate logistic Regression, Random Forest, and SVM\n",
    "- Poison detection via prediction disagreement\n",
    "- Evaluate poison impact and detection rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d39883",
   "metadata": {},
   "source": [
    "## Simulated Healthcare Dataset\n",
    "The code below creates a structured dataset to mimic diagnosis prediction from healthcare analysis data like lab tests, vitals, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87efcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate clean data\n",
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "X = np.random.normal(0, 1, (n_samples, 10))\n",
    "y = (X[:, 0] + X[:, 1] > 0).astype(int)  # synthetic label\n",
    "\n",
    "# Inject label poisoning in 10% of data\n",
    "poison_rate = 0.1\n",
    "n_poisoned = int(poison_rate * n_samples)\n",
    "y_poisoned = y.copy()\n",
    "poison_indices = np.random.choice(n_samples, n_poisoned, replace=False)\n",
    "y_poisoned[poison_indices] = 1 - y_poisoned[poison_indices]  # flip labels\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_poisoned, test_size=0.2, random_state=42)\n",
    "X_train_clean, _, y_train_clean, _ = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edba2fe",
   "metadata": {},
   "source": [
    "##  Ensemble Models and Prediction Disagreement\n",
    "Three models are trained and observed on how frequently they disagree, which is useful for detecting poison injections.\n",
    "This section also generates synthetic patient-like records with 10% label poisoning for testing of poison injection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab09305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble classifiers\n",
    "clf1 = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
    "clf2 = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "clf3 = SVC(probability=True).fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "pred1 = clf1.predict(X_test)\n",
    "pred2 = clf2.predict(X_test)\n",
    "pred3 = clf3.predict(X_test)\n",
    "\n",
    "# Count disagreement\n",
    "disagreements = (pred1 != pred2) | (pred1 != pred3) | (pred2 != pred3)\n",
    "disagreement_rate = np.mean(disagreements)\n",
    "print(f\"Disagreement rate: {disagreement_rate:.2f}\")\n",
    "\n",
    "# Train three different classifiers and analyze how often they disagree for poison detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c05ba",
   "metadata": {},
   "source": [
    "## Poison Detection via Disagreement\n",
    "High-disagreement samples are flagged and checked for how many samples were poisoned. Disagreements are analyzed to detect potential poisoned inputs based on the EPIC framework.\n",
    "Expected output: Disagreement increases on poisoned samples. High disagreement will flag anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map back to original test indices\n",
    "test_indices = np.arange(n_samples)[-len(X_test):]\n",
    "actual_poison_flags = np.isin(test_indices, poison_indices)\n",
    "predicted_poison_flags = disagreements\n",
    "\n",
    "# Calculate detection performance\n",
    "tp = np.sum(actual_poison_flags & predicted_poison_flags)\n",
    "fp = np.sum(~actual_poison_flags & predicted_poison_flags)\n",
    "fn = np.sum(actual_poison_flags & ~predicted_poison_flags)\n",
    "\n",
    "precision = tp / (tp + fp + 1e-9)\n",
    "recall = tp / (tp + fn + 1e-9)\n",
    "\n",
    "print(f\"Poison Detection Precision: {precision:.2f}\")\n",
    "print(f\"Poison Detection Recall: {recall:.2f}\")\n",
    "\n",
    "# Compute precision and recall for how well disagreement identifies poisoned records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347c44f",
   "metadata": {},
   "source": [
    "##  Accuracy Comparison\n",
    "Compare model performance on clean vs poisoned data.\n",
    "The EPIC detection method is simulated by flagging predictions where classifiers disagree.\n",
    "This code compares the performance of clean and poisoned models on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977f0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disagreement-based poison detection result\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Detected Poisons (TP)', 'Missed Poisons (FN)'], [tp, fn], color=['green', 'red'])\n",
    "plt.title('Poison Detection via Ensemble Disagreement')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deafd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on poisoned and clean train sets\n",
    "clf1_clean = LogisticRegression(max_iter=500).fit(X_train_clean, y_train_clean)\n",
    "acc_poisoned = accuracy_score(y_test, clf1.predict(X_test))\n",
    "acc_clean = accuracy_score(y_test, clf1_clean.predict(X_test))\n",
    "\n",
    "plt.bar(['Clean Model', 'Poisoned Model'], [acc_clean, acc_poisoned])\n",
    "plt.title('Model Accuracy (Clean vs Poisoned)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
